{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4934cb",
   "metadata": {},
   "source": [
    "# Time Series Forecasting with LSTM (PyTorch)\n",
    "\n",
    "This notebook walks through loading a time series from Yahoo Finance, transforming it into supervised windows, training a compact LSTM model to forecast future values, evaluating with common metrics, visualizing results, and saving the trained model. You will also learn how to inspect the model architecture and training logs using `torchinfo`, `torchviz`, and TensorBoard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98c75e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We build a univariate time‑series forecasting pipeline using PyTorch. The key steps are:\n",
    "\n",
    "- **Setup & data ingestion:** Install dependencies and fetch a daily closing price series via `yfinance` (or load a CSV).\n",
    "- **Preprocess:** Split the series into train/validation/test chronologically and scale it to [0, 1].\n",
    "- **Window the data:** Convert the 1D sequence into sliding windows of `LOOKBACK` timesteps to predict the next `HORIZON` value(s).\n",
    "- **Define the model:** Implement a small LSTM with a linear head.\n",
    "- **Train & visualize:** Train the model with early stopping, plot the training curves, and inspect the architecture.\n",
    "- **Evaluate & forecast:** Compute MAE, RMSE and MAPE on the test set, visualize predictions, and roll the model forward to forecast future values.\n",
    "- **Save & explore:** Export the trained model and scaler, view the model summary and computation graph, and launch TensorBoard to explore logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c2467",
   "metadata": {},
   "source": [
    "## Setup & Dependencies\n",
    "\n",
    "Install and import the core libraries needed for data handling, visualization, and building the LSTM in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854ebec",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Setup & Dependencies\n",
    "import sys, os, math, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Auto-install optional deps if missing (for Colab/clean envs)\n",
    "def _pip(pkg):\n",
    "    try:\n",
    "        __import__(pkg.split('==')[0])\n",
    "    except Exception:\n",
    "        get_ipython().system('pip -q install ' + pkg)\n",
    "\n",
    "_pip('yfinance>=0.2.40')\n",
    "_pip('scikit-learn>=1.3.0')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527763a",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Fetch a daily closing‑price series from Yahoo Finance using `yfinance`, or load your own CSV. Only one of the following code cells should be run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download a finance series with yfinance (default) { form-width: \"50%\" }\n",
    "import yfinance as yf\n",
    "TICKER = 'AAPL' #@param {type:\"string\"}\n",
    "START = '2010-01-01' #@param {type:\"string\"}\n",
    "END = ''  # empty = today #@param {type:\"string\"}\n",
    "interval = '1d'  # '1d','1wk','1mo'\n",
    "\n",
    "df = yf.download(TICKER, start=START or None, end=END or None, interval=interval, progress=False)\n",
    "assert not df.empty, f\"No data returned for {TICKER}. Try another ticker or date range.\"\n",
    "df = df[['Close']].rename(columns={'Close':'value'})\n",
    "df.index.name = 'date'\n",
    "df.head(), df.tail(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Alternative: Load a CSV from the internet (e.g., raw GitHub URL)\n",
    "# csv_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'  # example\n",
    "# df = pd.read_csv(csv_url)\n",
    "# # adapt columns if needed\n",
    "# if {'Date','Temp'}.issubset(df.columns):\n",
    "#     df = df.rename(columns={'Date':'date','Temp':'value'})\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df = df.set_index('date')\n",
    "# df.head(), df.tail(), df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b41cc",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split & Scaling\n",
    "\n",
    "Chronologically split the series into training, validation, and test sets (70%/15%/15%) to preserve temporal order. Then scale the values to the [0, 1] range using `MinMaxScaler` so that the LSTM trains more stably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c023d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Split & scale\n",
    "values = df[['value']].astype('float32').values\n",
    "n = len(values)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(values[:train_end])\n",
    "val_scaled   = scaler.transform(values[train_end:val_end])\n",
    "test_scaled  = scaler.transform(values[val_end:])\n",
    "\n",
    "len(train_scaled), len(val_scaled), len(test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122bd905",
   "metadata": {},
   "source": [
    "## Windowing the Series\n",
    "\n",
    "Transform the 1D time series into supervised learning samples. Each sample consists of a window of `LOOKBACK` past values and the next `HORIZON` value(s) as targets. This function creates the input/output arrays needed for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Window utilities\n",
    "from typing import Tuple\n",
    "\n",
    "def make_windows(arr: np.ndarray, lookback: int, horizon: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X, y = [], []\n",
    "    for i in range(len(arr) - lookback - horizon + 1):\n",
    "        X.append(arr[i : i+lookback])\n",
    "        y.append(arr[i+lookback : i+lookback+horizon])\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "LOOKBACK = 60   #@param {type:\"integer\"}\n",
    "HORIZON  = 1    #@param {type:\"integer\"}\n",
    "\n",
    "X_train, y_train = make_windows(train_scaled, LOOKBACK, HORIZON)\n",
    "X_val,   y_val   = make_windows(val_scaled, LOOKBACK, HORIZON)\n",
    "X_test,  y_test  = make_windows(test_scaled, LOOKBACK, HORIZON)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d9288",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader\n",
    "\n",
    "Wrap the windowed arrays into a PyTorch `Dataset` and `DataLoader` so batches can be fed into the model during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Dataset & DataLoader\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH_SIZE = 64  #@param {type:\"integer\"}\n",
    "train_ds = SeqDataset(X_train, y_train)\n",
    "val_ds   = SeqDataset(X_val, y_val)\n",
    "test_ds  = SeqDataset(X_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718311d",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "Build a lightweight LSTM network with a configurable number of layers and hidden units. A linear layer on top projects the final hidden state to the desired forecast horizon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb110dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title LSTM model\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.1, horizon=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Linear(hidden_size, horizon)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "model = LSTMForecaster(horizon=HORIZON).to(DEVICE)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ef77e",
   "metadata": {},
   "source": [
    "### Model Visualization\n",
    "\n",
    "After defining the model, use two optional tools to inspect its architecture:\n",
    "\n",
    "1. **`torchinfo` summary** – prints a table of each layer with output shapes and parameter counts.\n",
    "2. **`torchviz` computation graph** – renders a PNG of the forward computation graph.\n",
    "\n",
    "If these tools are not already installed, the first code cell below will install them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install visualizers\n",
    "try:\n",
    "    import torchinfo  # noqa: F401\n",
    "    from torchviz import make_dot  # noqa: F401\n",
    "except Exception:\n",
    "    # Colab/clean envs\n",
    "    import sys\n",
    "    !pip -q install torchinfo torchviz graphviz\n",
    "    import torchinfo\n",
    "    from torchviz import make_dot\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "artifacts = Path('artifacts'); artifacts.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ec9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title torchinfo summary (table view)\n",
    "from torchinfo import summary\n",
    "\n",
    "assert 'model' in globals(), 'Model not found. Run the model definition cell first.'\n",
    "assert 'LOOKBACK' in globals(), 'LOOKBACK not found.'\n",
    "\n",
    "s = summary(model, input_size=(1, LOOKBACK, 1),\n",
    "            col_names=(\"kernel_size\",\"num_params\",\"mult_adds\",\"trainable\"),\n",
    "            depth=6, verbose=0)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05803700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title torchviz computation graph (PNG)\n",
    "assert 'model' in globals(), 'Model not found. Run the model definition cell first.'\n",
    "assert 'LOOKBACK' in globals(), 'LOOKBACK not found.'\n",
    "assert 'DEVICE' in globals(), 'DEVICE not found.'\n",
    "\n",
    "model.eval()\n",
    "dummy = torch.randn(1, LOOKBACK, 1, device=DEVICE)\n",
    "out = model(dummy)\n",
    "dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "png_path = str((artifacts / 'model_graph').resolve())\n",
    "dot.render(png_path, cleanup=True)\n",
    "display(Image(filename=png_path + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bd646",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the LSTM using mean squared error (MSE) loss, the AdamW optimizer, and a cosine learning rate scheduler. Early stopping monitors the validation loss to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train loop with early stopping\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "EPOCHS = 50  #@param {type:\"integer\"}\n",
    "LR = 1e-3    #@param {type:\"number\"}\n",
    "PATIENCE = 7 #@param {type:\"integer\"}\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(EPOCHS, 10))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_state = None\n",
    "pat = 0\n",
    "train_hist, val_hist = [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.squeeze(-1).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss /= len(train_ds)\n",
    "    train_hist.append(epoch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vloss = 0.0\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.squeeze(-1).to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            vloss += loss.item() * xb.size(0)\n",
    "        vloss /= len(val_ds)\n",
    "        val_hist.append(vloss)\n",
    "\n",
    "    scheduler.step()\n",
    "    if vloss < best_loss:\n",
    "        best_loss = vloss\n",
    "        best_state = model.state_dict()\n",
    "        pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "    if (epoch % 5) == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | train {epoch_loss:.5f} | val {vloss:.5f} | lr {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    if pat >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "else:\n",
    "    print(\"Warning: best_state was None; using last epoch state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b95d5",
   "metadata": {},
   "source": [
    "### Visualize Training History\n",
    "\n",
    "Plot the training and validation loss curves across epochs to see how the model converges and when early stopping is triggered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14524cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot training curves\n",
    "plt.figure()\n",
    "plt.plot(train_hist, label='train')\n",
    "plt.plot(val_hist, label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f7f27",
   "metadata": {},
   "source": [
    "### Explore Training Logs with TensorBoard\n",
    "\n",
    "This cell logs the actual training and validation losses collected in `train_hist` and `val_hist` during the LSTM training run. It then launches TensorBoard so you can interactively explore the loss curves and other metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d32a5b-df6e-4063-95af-e4306001ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy-aware TensorBoard launcher (works on JupyterHub/JupyterLab)\n",
    "# Log the actual training and validation losses collected during the LSTM training run\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logdir = \"runs/lstm_training\"\n",
    "writer = SummaryWriter(logdir)\n",
    "# Write training and validation losses\n",
    "for epoch, (train_loss, val_loss) in enumerate(zip(train_hist, val_hist)):\n",
    "    writer.add_scalar(\"Loss/train\", float(train_loss), epoch)\n",
    "    writer.add_scalar(\"Loss/val\", float(val_loss), epoch)\n",
    "writer.close()\n",
    "\n",
    "# Start TensorBoard on a free port and embed via the hub prefix\n",
    "import os, re\n",
    "from tensorboard import program\n",
    "from IPython.display import HTML\n",
    "\n",
    "tb = program.TensorBoard()\n",
    "tb.configure(argv=[None, \"--logdir\", logdir, \"--load_fast=false\", \"--bind_all\", \"--port\", \"0\"])\n",
    "tb_url = tb.launch()                     # e.g. http://127.0.0.1:6007/\n",
    "print(\"TensorBoard (direct):\", tb_url)\n",
    "\n",
    "# Construct the proxied URL for JupyterHub/Lab environments\n",
    "port = re.search(r\":(\\d+)\", tb_url).group(1)\n",
    "prefix = os.environ.get(\"JUPYTERHUB_SERVICE_PREFIX\", \"/\")\n",
    "iframe_url = f\"{prefix.rstrip('/')}/proxy/{port}/\"   # e.g. /user/<id>/proxy/6007/\n",
    "HTML(f'<iframe src=\"{iframe_url}\" width=\"100%\" height=\"620\" style=\"border:0\"></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc141d7",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n",
    "\n",
    "Invert the scaling on predictions and compute common regression metrics such as MAE, RMSE, and MAPE. If predicting multiple steps, metrics are averaged across steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluate on test\n",
    "model.eval()\n",
    "preds_list, trues_list = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        preds = model(xb).cpu().numpy()          # (B, H)\n",
    "        trues = yb.squeeze(-1).cpu().numpy()     # (B, H)  <-- squeeze extra dim\n",
    "        preds_list.append(preds)\n",
    "        trues_list.append(trues)\n",
    "\n",
    "preds = np.concatenate(preds_list, axis=0)  # (N, H)\n",
    "trues = np.concatenate(trues_list, axis=0)  # (N, H)\n",
    "\n",
    "# invert scaling (works for 2-D (N,H))\n",
    "def inverse_scale_2d(mat: np.ndarray) -> np.ndarray:\n",
    "    return np.hstack([scaler.inverse_transform(mat[:, [i]]) for i in range(mat.shape[1])])\n",
    "\n",
    "preds_inv = inverse_scale_2d(preds)\n",
    "trues_inv = inverse_scale_2d(trues)\n",
    "\n",
    "mae = mean_absolute_error(trues_inv.flatten(), preds_inv.flatten())\n",
    "rmse = mean_squared_error(trues_inv.flatten(), preds_inv.flatten())\n",
    "mape = np.mean(np.abs((trues_inv - preds_inv) / (np.clip(np.abs(trues_inv), 1e-8, None)))) * 100\n",
    "\n",
    "print({'MAE': float(mae), 'RMSE': float(rmse), 'MAPE_%': float(mape)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac3d53",
   "metadata": {},
   "source": [
    "## Visualize Last Window Predictions\n",
    "\n",
    "Compare the model's predictions with the ground truth on the last few points of the test set to get a feel for forecasting accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073eb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot predictions vs actuals (tail)\n",
    "k = 200  #@param {type:\"integer\"}\n",
    "plt.figure()\n",
    "plt.plot(trues_inv.flatten()[-k:], label='Actual')\n",
    "plt.plot(preds_inv.flatten()[-k:], label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(f'Last {k} steps: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1d2f9",
   "metadata": {},
   "source": [
    "## Multi-step Forecasting\n",
    "\n",
    "Roll the model forward by repeatedly feeding its own predictions back into the input window. This allows forecasting several steps into the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast next steps (business/day-aware + adjustable length)\n",
    "FORECAST_STEPS = 90  # make it longer if you want\n",
    "\n",
    "series_scaled = scaler.transform(values)\n",
    "context = series_scaled[-LOOKBACK:].copy()\n",
    "outs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(FORECAST_STEPS):\n",
    "        xb = torch.tensor(context[None, :, :], dtype=torch.float32, device=DEVICE)\n",
    "        pred = model(xb).cpu().numpy()[0]\n",
    "        step = pred[-1] if HORIZON > 1 else pred[0]\n",
    "        outs.append(step)\n",
    "        context = np.concatenate([context[1:], np.array([[step]], dtype=np.float32)], axis=0)\n",
    "\n",
    "forecast = scaler.inverse_transform(np.array(outs).reshape(-1, 1)).flatten()\n",
    "\n",
    "# use the series' actual time step (handles business days)\n",
    "delta = df.index[-1] - df.index[-2]\n",
    "future_index = pd.date_range(df.index[-1] + delta, periods=FORECAST_STEPS, freq=delta)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df.index[-200:], df.value[-200:], label='History (tail)')\n",
    "plt.plot(future_index, forecast, label='Forecast')\n",
    "plt.title('Tail + Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ffec4",
   "metadata": {},
   "source": [
    "## Save Artifacts\n",
    "\n",
    "Persist the trained model and the scaler to disk so that you can reload them later for inference without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64909db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Export model & scaler\n",
    "out_dir = Path('artifacts'); out_dir.mkdir(exist_ok=True)\n",
    "model_path = out_dir / 'lstm_forecaster.pt'\n",
    "scaler_path = out_dir / 'scaler.pkl'\n",
    "torch.save({'state_dict': model.state_dict(), 'hparams': {'lookback': LOOKBACK, 'horizon': HORIZON}}, model_path)\n",
    "import pickle\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "model_path, scaler_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b4d57",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Switch to a different ticker or interval for other time scales.\n",
    "- For multiple features (e.g., OHLCV), expand input_size and adapt the scaler.\n",
    "- For seasonality, consider adding Fourier terms or using `Prophet`/`SARIMAX` for baselines.\n",
    "- For long contexts, try `Temporal Convolutional Networks` or `Transformer` variants.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Time_Series_LSTM_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CamberPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
