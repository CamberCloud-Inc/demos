{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cc10c5-d259-416c-85fe-ce9793b2ddc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T00:32:57.051972Z",
     "start_time": "2025-06-03T00:32:57.050564Z"
    }
   },
   "source": [
    "# 1. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ce105-9025-468e-bc85-a67a6961b7b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:53:25.848695Z",
     "start_time": "2025-06-04T19:53:25.819643Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257cd11-4ca6-465c-a54f-e3713308ff28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:53:25.848695Z",
     "start_time": "2025-06-04T19:53:25.819643Z"
    }
   },
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37122f-9d33-4596-bf83-f2adbcbd3ae5",
   "metadata": {},
   "source": [
    "# 2. BASIC SETUP - Simple function to talk to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a99e2f-d7b8-4010-a4fe-601cb9fbcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ollama.ollama.svc.cluster.local:11434\"\n",
    "client = Client(\n",
    "  host=url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ae819-9083-4156-9b48-69c123aef741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(prompt, model=\"gemma3:4b\", stream=True):\n",
    "    \"\"\"Simple function to send a prompt to Ollama and get response\"\"\"\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "    final_output = \"\"\n",
    "    try:\n",
    "        response = client.chat(model=model, stream=stream, messages=messages)\n",
    "        if stream:\n",
    "            for chunk in response:\n",
    "                chunk_content = chunk.message.content\n",
    "                final_output += chunk_content\n",
    "                print(chunk_content, end='', flush=True)\n",
    "            print()\n",
    "            return final_output\n",
    "        else:\n",
    "            print(response.message.content)\n",
    "            final_output = response.message.content\n",
    "            return final_output\n",
    "    except Exception as e:\n",
    "        print(f\"Connection error: {e}\")\n",
    "        return f\"Connection error: {e}\"\n",
    "\n",
    "\n",
    "# Test the connection\n",
    "print(\"Testing connection to Ollama...\")\n",
    "response = ask_llm(\"Say hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72698f60-b473-4171-a6d3-5708e4048e98",
   "metadata": {},
   "source": [
    "# 3. BASIC TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e60182-0efe-45de-b9f3-2994db0108cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO 1: Basic Text Generation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simple question answering\n",
    "print(f\"Question: What is Python programming language?\")\n",
    "response = ask_llm(\"What is Python programming language?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cdc61-5e03-4eca-9aa8-1d956550a83e",
   "metadata": {},
   "source": [
    "# 4. CODE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cfd9f-884d-45f3-ae33-3d661b779113",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO 2: Code Generation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "code_prompt = \"Write a Python function to calculate the area of a circle\"\n",
    "print(f\"Request: {code_prompt}\")\n",
    "code_response = ask_llm(code_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf600c9-e51b-46f7-8820-005d8bb68e96",
   "metadata": {},
   "source": [
    "# 5. TEXT SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f4abf-486e-455c-bd3c-a2c6f9824e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO 3: Text Summarization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to \n",
    "natural intelligence displayed by animals including humans. AI research has been \n",
    "defined as the field of study of intelligent agents, which refers to any system that \n",
    "perceives its environment and takes actions that maximize its chance of achieving its \n",
    "goals. The term artificial intelligence is often used to describe machines that mimic \n",
    "cognitive functions that humans associate with the human mind, such as learning and \n",
    "problem solving. Modern AI techniques have experienced a resurgence following \n",
    "concurrent advances in computer power, large amounts of data, and theoretical \n",
    "understanding; and AI techniques have become an essential part of the technology \n",
    "industry, helping to solve many challenging problems in computer science, software \n",
    "engineering and operations research.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = f\"Summarize this text in 2-3 sentences:\\n\\n{long_text}\"\n",
    "print(f\"Summary:\")\n",
    "summary = ask_llm(summary_prompt)\n",
    "print(f\"Original text length: {len(long_text)} characters\")\n",
    "print(f\"Summary length: {len(summary)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cefb1-7d8e-4f20-8b94-3c016ea12114",
   "metadata": {},
   "source": [
    "# 6. QUESTION ANSWERING WITH CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe61c8d-cad3-4869-8d6b-cefef2e0413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO 4: Question Answering with Context\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "context = \"\"\"\n",
    "The Titanic was a British passenger liner that sank in the North Atlantic Ocean \n",
    "on 15 April 1912, after striking an iceberg during her maiden voyage from \n",
    "Southampton to New York City. There were an estimated 2,224 passengers and crew \n",
    "aboard, and more than 1,500 died, making it one of the deadliest commercial \n",
    "peacetime maritime disasters in modern history.\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = f\"\"\"\n",
    "Context: {context}\n",
    "\n",
    "Question: How many people died in the Titanic disaster?\n",
    "\n",
    "Answer based on the context provided:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Context provided about Titanic\")\n",
    "print(f\"Question: How many people died in the Titanic disaster?\")\n",
    "print(f\"Answer:\")\n",
    "answer = ask_llm(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13960f-9d58-4f60-bd84-631cee299b8d",
   "metadata": {},
   "source": [
    "# 7. COMPARING DIFFERENT PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b9171-af5d-46e1-9429-093d80685cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO 5: Prompt Engineering - Comparing Different Approaches\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Bad prompt\n",
    "bad_prompt = \"Explain ML\"\n",
    "\n",
    "print(\"BAD PROMPT:\")\n",
    "print(f\"Prompt: '{bad_prompt}'\")\n",
    "print(f\"Response:\")\n",
    "bad_response = ask_llm(bad_prompt)\n",
    "\n",
    "# Good prompt\n",
    "good_prompt = \"Explain machine learning in simple terms that a beginner can understand. Include what it is, how it works, and give one practical example.\"\n",
    "\n",
    "print(\"\\nGOOD PROMPT:\")\n",
    "print(f\"Prompt: '{good_prompt}'\")\n",
    "print(f\"Response:\")\n",
    "good_response = ask_llm(good_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb445f0b-d3f0-4635-ac5d-93b869f312e6",
   "metadata": {},
   "source": [
    "# 8. INTERACTIVE DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bebfa-ca60-435a-a32c-117205e28dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_demo():\n",
    "    \"\"\"Interactive function for live demonstration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LIVE DEMO - Ask the LLM anything!\")\n",
    "    print(\"Type 'quit' to stop\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour question: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        print(f\"AI Response:\")\n",
    "        ask_llm(user_input)\n",
    "\n",
    "# Uncomment the line below for interactive demo during presentation\n",
    "live_demo()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Key takeaways:\")\n",
    "print(\"1. LLMs can handle various tasks: Q&A, coding, summarization, creative writing\")\n",
    "print(\"2. Better prompts lead to better responses\")\n",
    "print(\"3. LLMs can work with provided context\")\n",
    "print(\"4. Local models via Ollama keep your data private\")\n",
    "print(\"5. Easy to integrate into Python applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e5df3-4fde-4cc8-98d6-182f4c30a481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CamberPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
